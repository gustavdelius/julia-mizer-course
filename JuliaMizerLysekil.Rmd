---
title: "Confronting Size Spectrum Theory with Data"
output: pdf_document
author: Julia Blanchard
date: 05/07/2019
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Lysekil Training Course, Sweden, Sept. 6-9, 2019

#### Asking Questions with Size Spectrum Models

#### Julia L. BLanchard

Empiricists have been reporting changes in the size spectrum for many years, such as Sheldon and colleague's early observations (Sheldon et al). Even prior to that Ghilarov reported that log biomass would be equivalent in log size classess - we he showed for soil organisms - at large scales (Polishchuk ref). Fisheries scientists have been analysing trends in the fish community size spectrum for many years as well and have examined its' potential use as an indicator for the ecosystem effects of fishing (Jennings et al., Shin et al. 2005, Daan et al. 2005, Blanchard et al. 2005 etc.). This is because the slope of the "fished"" component of the size spectrum been observed to become steeper through time - an effect the loss of larger individuals through fishing mortality. The intercept of teh size spectrum on a log-log scale is alos important and is thought to be influenced by the amount and production of smaller primary producers in the ecosystem (Bianchi et al. 2001).

In this section of the course we will explore how we can learn about models by fitting size spectrum ecological models to data using the "mizer" R package. Later on, we will use "calibrated" model(s) to explore the dynamics of the system through time and carry out fishing (and climate change) scenarios. 

Recall, there are three different kinds of size spectrum models in mizer, of increasing complexity: 
1) community model: purely size-based and representative of a single but  "average" species across the whole community 
2) trait-based model, which disaggregates the size spectrum into differentgroups with different life-histories, through differences in each "species" asymptotic which determines
other life-history parameters such as the size at maturity (Hartvig et al. 2011, Andersen & Pedersen, 2010)
3) multispecies model - which has the same equations and parameters as the trait-based model but is parameterised to represent multiple species in a real system, where each species can have many differing species-specific traits (Blanchard et al. 2014). 
4) scale-free model (Gustav can explain that one!)

Here, we will focus mostly on  multispecies models but the same approach can be carried out with the other models. Actually since these are general approaches, they can be used with *any* deterministic mechanistic models. To start with though we will consider confronting the simplest model - the community model - with data on the community size spectrum (What is a size spectrum? See Ken's section!).

###### Example #1 - Fitting the community model to size spectrum data

```{r}
#get required packages
require(mizer)
require(tidyverse)
#lets's read in some north sea community size spectrum data
css<-read.table("sizespecobs.txt",header=T)
#select data for year 2000
css <- css %>%
  filter(year == "2000", Nden > 0) %>%
  select(midw, Nden)
#have a look at plot
plot(log(css$midw),log(css$Nden),xlab="midpoint of weight class (g)", ylab="number density")
# we can fit a smoother to these points to generate easy access "observations" to compare with model later
lo<-loess(log(css$Nden)~log(css$midw),span=0.2)
points(log(css$midw),predict(lo),typ="l")
```

The first question we will pose here is: How do we fit a simple community size spectrum model to community size spectrum data? To answer this we ask: What do the community size spectrum model parameters need to be to best capture observations of the community size spectrum? We will consider only two parameters - the level of fishing effort (here, simply a multiplier of fishing mortality rate) and the background resource carrying capacity which are known to affect the size spectrum.

First let's set up the communiy size spectrum model and plot it.

```{r}
params <- set_community_model(knife_edge_size = 10)
sim <- project(params, effort = 0, t_max = 100, dt=0.1, dx=200)
plotSpectra(sim)
```

We could stop here and simply compare the modelled size spectrum slopes and intercepts to the observed ones shown above. But instead  we are going to go a bit further  - but starting simple -  and simply run a range of parameter values, for *effort* and *kappa*. 


So, we want the model to be able to run over a range of parameters that we pass to it. 

```{r}
# set up function to run model and output the predicted size spectrum
runmodel<-function(parms){# set up and run!
params <- set_community_model(knife_edge_size = 10,kappa=parms$kappa)
sim <- project(params, effort = parms$effort, t_max = 200, dt=0.1)
# select last 10 years of output (should be time-averaged)
# slope only
# output<-mean(getCommunitySlope(sim,min_w=16,max_w=1600)[190:200,1])
# or whole size spectrum
output <-apply(sim@n,c(2,3),mean)
return(output)
}
```

We will interrogate the outputs that best "fit" the data. First we need to specify what we mean by our "best fit".
We will use simple least squares regression and assess the sum of squared errors (SSE) between the observed and the modelled size spectra. We will select and examine the parameter set with the lowest SSE.

```{r}
# set up some initial parameters
parms=data.frame(kappa=0.1,effort=1)
# get data for same weight values
logw <- log(params@w[which(params@w  >= 16 & params@w <= 22000)])
dat <- predict(lo,logw)

# set up error function to compare predictions with observations
sse <- function(parms,dat) {
pred <- log(runmodel(parms)[which(params@w  >= 16 & params@w <= max(css$midw))])
# sum of squared errors
discrep <- pred - dat
return(sum(discrep^2))
}
err<-sse(parms,dat)
#test
err
```

We could skip ahead to  optimisation here to simply find the best parameter set. Instead, to illustrate how this works and to examine the likelihood surface, we will set up a simple grid of parameters. Because the models runs are not actually dependent on each other (they are sequential), we can also do this more quickly with parallel computing.

```{r}
f <- function (par) {
parms$kappa <- par[1]
parms$effort <- par[2]  
sse(parms,dat)
}
# single parameter
kappa <- seq(from=0.05,to=0.1,by=0.05)
effort <- seq(from=0,to=2,by=0.1)
grid <- expand.grid(kappa=kappa,effort=effort)
grid$SSE <- apply(grid,1,f)
```

Let's have a look at the error surface and extract the parameter set that gives the minimum error.

```{r}
# which level of effort has overall least error?
effort.hat <- grid$effort[which.min(grid$SSE)]
kappa.hat <- grid$kappa[which.min(grid$SSE)]
# Basic scatterplot
ggplot(grid, aes(x=effort, y=SSE,col=kappa) ) +
  geom_point() 

```

What do these results suggest about the influence of kappa on the effects of fishing? Next plug the effort.hat and kappa.hat values back into the model and plot the modelled and observed size spectra.

```{r}
params <- set_community_model(knife_edge_size = 10,kappa=kappa.hat)
sim <- project(params, effort = effort.hat, t_max = 100, dt=0.1)
w <-dimnames(sim@n)$w
n <-sim@n[100,,]
plot(w,n,log="xy",typ="l",xlim=c(0.1,30000))
#add the data to the plot
points(css$midw,css$Nden, col= "steel blue")

```

### Exercise #1

The model predictions do pass through the points but the shape of the size spectra look a bit different. Think about wheter we are comparing "like with like" in terms of model and data. Would we get similar results if we just used the modelled and observed size size spectrum slopes and intercepts? What happens if we vary different parameters and different values? 


##### Alternate Example # 1 or Example 2: Fantasy multispecies model and data

Let's set up a multispecies model in mizer. This is a fantasy ecosystem so anything goes! These parameters should be familiar from the earlier theory presentations of the trait-based model and we will explain them in more detail in the "Parameterisation" section. Feel free to change them!

## Species-specific parameters
```{r}
require(mizer)
fic.params<-data.frame(matrix(0,5,0))
fic.params$species<-c("Blinky","Fishtronaut", "Fluffy","Mermaids","Nessie")
fic.params$w_inf<-c(400,100,3000,40000,100000)
fic.params$w_mat<-c(100,20,500,10000,20000)
fic.params$z0<-c(0.2,0.2,0.14,0.12,0.25) ## mu_0
fic.params$r_max<-c(1e8,1e9,5e6,1e3,1e4)
fic.params$beta<-c(3210,42100,3200,200,30)
fic.params$sigma<-c(1.2,1.4,1.8,2,3)
fic.params$ks<-c(1,1,0.9,0.91,0.54)
fic.params$h<-c(10,14,30,20,32)
fic.params$gamma<-c(1e-9,2e-11,1e-11,6e-10,7e-12)

# interaction matrix
fic.inter<-matrix(1,5,5)
colnames(fic.inter)<-rownames(fic.inter)<-fic.params$species
fic.inter[1,1]<-1;fic.inter[1,2]<-0.8;fic.inter[1,3]<-0.4;fic.inter[1,4]<-0.9;fic.inter[1,5]<-0.8
fic.inter[2,1]<-0.53;fic.inter[2,2]<-0.35;fic.inter[2,3]<-0.6;fic.inter[2,4]<-0.8;fic.inter[2,5]<-0.42
fic.inter[3,1]<-1;fic.inter[3,2]<-0.76;fic.inter[3,3]<-0.5;fic.inter[3,4]<-1;fic.inter[3,5]<-0.72
fic.inter[4,1]<-0.05;fic.inter[4,2]<-0.8;fic.inter[4,3]<-0;fic.inter[4,4]<-0;fic.inter[4,5]<-0.8
fic.inter[5,1]<-1;fic.inter[5,2]<-0.74;fic.inter[5,3]<-1;fic.inter[5,4]<-1;fic.inter[5,5]<-0.9;

# set up and run!
params.fic <- MizerParams(fic.params, fic.inter,r_pp=0.1,lambda=2.2)
sim.fic <- project(params.fic, t_max = 200,dt=0.1)
# show the plot
plot(sim.fic)

```

The plot shows there are some pretty cool size distributions, and other than the largest sea monster (Nessie) struggling to eat enough food, the biomass of each species in the model is approaching a steady state.

Just for fun, let's have a look at who is eating whom!

```{r}
#ADD NEW DIET PLOT HERE :)
```

Now, let's make up some data by adding a bit of normally distrubuted error around the model predictions from the last time step. We can then calculate the error between the "observed" and predicted.

```{r}
pred <- getBiomass(sim.fic)[200,]
names(pred) <- names(getBiomass(sim.fic)[200,])
dat <- as.matrix(read.table("ficdata.txt"))
plot(dat,pred,xlab="observed",ylab="predicted",log="xy")
text(dat,pred,names(pred),cex=0.6,col="tomato")
abline(0,1)

```

To start, we will keep things very simple and just vary one or two parameters. We will focus on a highly uncertain specie-specific parameter: Rmax (essentially the carrying capacity of recruits). This parameter is handy as it scales the biomass and abundances for each species. Here we have a simple goal that we'd like to be in the right ball park of the time-averaged biomass values for each species. 

First, we can wrap MizerParams() and project() functions into a function called runmodel() so that we can easily call this later. 

```{r}
runmodel<-function(params, inter){# set up and run!
params.fic <- MizerParams(params, inter)
sim.fic <- project(params.fic, t_max = 200,dt=0.1)
# use the same time slot (should be time-averaged)
output<-getBiomass(sim.fic)[200,]
return(output)
}

```

Next, we  want to calculate the discrepancy between model and data. Again, we will use simple *least squares* to calculate the sum of squared errors between model and data. First, we will build a function to calculate the discrepancy between the model and data.

```{r}
sse <- function(params,inter, dat) {
pred <- runmodel(params, inter)
discrep <- pred - dat
return(sum(discrep^2))
# sum of squared errors
}
err<-sse(fic.params,fic.inter,dat)
err
```

The biomasses of Mermaids and Nessie are not very realistic in terms of distance from the observations. Let's work out whether we can find a better estimates of Rmax for these two species, simply by varying the values of Rmax, relative to the range of default values. 

[Side note for later: where do these default Rmax values come from? Ken will tell you!]

```{r}
f <- function (new_rmax) {
fic.params[3,"r_max"] <- new_rmax
print(sse(fic.params,fic.inter,dat))
print(fic.params[3,"r_max"])
}
#latin hypercube sampling or optim?
new_rmax <- exp(log(10)*seq(log10(1e0),log10(1e11),by=0.5)) 
SSE <- sapply(new_rmax,f)

```

Our new estimate of Rmax will be the one with the lowest SSE.

```{r}
rmax.hat <- new_rmax[which.min(SSE)]
plot(new_rmax,SSE,typ="l")
```

It seems like our best estimate results in a model with a lot less Fluffy! Take a few minutes to discuss *Why?* 

```{r}
fic.params[3,"r_max"] <- rmax.hat
params.fic <- MizerParams(fic.params, fic.inter,r_pp=0.1,lambda=2.2)
sim.fic <- project(params.fic, t_max = 200,dt=0.1)
# show the plot
plot(sim.fic)
#check out fit
pred<-getBiomass(sim.fic)[200,]
plot(dat,pred,xlab="observed",ylab="predicted",log="xy")
abline(0,1)
```

#### Example # 2 or 3  - Time-averaged Optimisation OR Time-series Optimisation

That was just for our given set of fantasy model parameters and some made up data - a fantasy indeed. Real ecology is so much messier! Also we would like to do better than just evaluating what the smallest error is for one species' Rmax, across a range of values.  We would like to learn about what values the model parameters would need to be to best capture the data. We can optimise all of the species' Rmax parameters simultaneously to see how well we can fit the data.  This time we will use the North Sea model - but instead of using the provided calibrated North Sea Rmax values in mizer from Blanchard et al. 2014 (who tried their best at the time...) we will reset the Rmax values and redo the analysis using a few new packages in R. Spoiler alert: we don't get the same values.

First, let's check out the model using using the default Rmax values.

```{r}
data(NS_species_params)
data(inter)
params <- MizerParams(NS_species_params, inter)
#run model
sim <- project(params, t_max = 200, effort = 1,dx=200,dt=0.1)
plot(sim)
```

Many statisticians have devoted their time to developing to optimization algorithms, and there are many of these. Many are implemented in R. We used optim (combined with brute force in the form of penaly functions!) for the Blanchard et al. 2014 paper. Since then, things have moved on a bit in the fast paced world of computation and R. Here we use essentially the same simple least squares method, but we use the package nloptr. 


We will move away from the default values and attempt to gain inference about the Rmax values from the catch data. First we will need to rewrite our functions to ensure we pass the optimozer the correct data and parameters.

```{r}
runmodel<-function(params, inter){ 
# set up and run!
paramsr <- MizerParams(params, inter)
sim <- project(paramsr, t_max = 200,dt=0.1, effort = 1)
# use the same time slot (should be time-avaeraged)
output<-colMeans(getYield(sim)[150:200,])/1e6
return(output)
}
```


To use noptlr ( or optim or another package)  we need to specify the function we want to minimize and the initial values for the parameters. Starting from this point, the package's algorithms will search the parameter space for the value that minimizes the value of our objective function (sse).

First, we will write an objective function to try to estimate the Rmax values for all 12 species simultaneously. We'll work on the log-transformed values for Rmax parameters as well as the observed catches. Note that Blanchard et al. 2014 used catches and SSB from stock assessment models to estimate Rmax and kappa. Here, to avoid the model bias we will use catches only (and as in Spence et al.2016). 

```{r,warning=T}
#get data on catches over time-averaged period
catch <- read.csv("catches.csv")
dat <- catch[,"Catch_8595"]
#make up a new set of rmax values, here that scales with species' w_inf
NS_species_params$r_max <- 1e11*NS_species_params$w_inf^-1.2

#set up error function
sse <- function(params, dat,inter) {
pred <- runmodel(params,inter)
# mean squared logarithmic error
discrep <- log(dat+1) - log(pred+1)
#discrep <- dat - pred
return(sum(discrep^2)/length(discrep))
# sum of squared errors
}
#test for 1 run
error<-sse(NS_species_params,dat,inter)

```

We then pass the parameters to the functions and run the optimiser - and hope for the best! Let's take a break here...

```{r}
f <- function (par) {
NS_species_params$r_max <- exp(par)
sse(params,dat,inter)
}
optim(fn=f,par=log(NS_species_params$r_max),method=c("SANN"), control=list(trace=1)) -> fit

#start with global optoimization
nloptr(x0=log(NS_species_params$r_max),lb=rep(log(1e3),12), ub=rep(log(1e12),12), eval_f=f,opts=list("algorithm" =
"NLOPT_GN_DIRECT_L","xtol_rel"=1.0e-4)) -> fit

#then use these in a local optimisation scheme
nloptr(x0=log(NS_species_params$r_max),lb=rep(log(1e3),12), ub=rep(log(1e12),12), eval_f=f,opts=list("algorithm" =
"NLOPT_LN_COBYLA","xtol_rel"=1.0e-4)) -> fit
```

Now let's see how these optimised values  of Rmax the time-averaged model fits. 
```{r}
params@species_params$r_max <- fit$solution
#re-run model to check
sim <- project(params, t_max = 200, effort = 1,dx=200,dt=0.1)
plot(sim)
# check pred-obs plot
pred<-colMeans(getYield(sim)[100:200,])/1e6
plot(dat,pred)
abline(0,1)
```

As you can see this is not the most convincing fit! Only a couple of the values actually change. This is suspicious and likely means the model is stuck in a local minimum.

##### Reducing the parameter space 

An alternative to running the optimiser and model with initial values is to reduce the parameters space to those values that actually lead the model to pluaisble, and ecolgically accpetable model outputs. This may be achieved form careful hand-tuning and "pattern-oriented" visual examination  (ref, see Gustav's RShiny example) to achive initial values that are very close to those "optimal values". However, it may also be the case that we would like to learn what range of values can lead to plausible.

### History matching

We use Latin Hypercube Sampling to randomly sample the parameter space, folllowed by a combination of least squares and history matching (Clark et al. 2003) to evaluate the parameter sets.

We evaluate the plausibility of these paramters by running the model and evaluationg some criteria. These can be designed to be specific to your study system and reserach questions. For this example, we simply aim for the species in the system to be:
i) within the range of observed catches (+/- 20%)
ii) for species to still coexist when fishing is set to 0
iii) estimated growth parameters are close to empirical estimates. 

[Note: we also can use Gustav's RShiny app to examine these outputs].

All parameter sets that do not meet these criteria are implausible and are hence REJECTED. This provides us with a new subset of the parameter space within which we can proceed to sample again via optimisation (with bootstrapping if we wish to examine uncertainies around these values) and/or more sophisticated Bayesian or Approximate Bayesian Computation (Toni et al 2009) techniques to formally examine parameter uncertainity including time-series fitting (Spence et al. 2016, Spence et al. in review).

#### Latin hypercube sampling


#### Run the rejection algorithm for all of the saved output files and add column of "reject" variable to input table


######### Dynamics: Modelling Changes in Fish Communities Through Time

#### TO ADD Example 3 - How does the community change through time in response to changes in fishing. How can we model changes through time from fishing?  Unless already shown in time series calibration above (in which case can be an examination of scenario projection after time series).

#### TO ADD Example 4 - How can we model changes through time from climate change? Show example run and how to set up model and force with temperature and/or background resource changes through time (we can use extended mizer version "thermizer" for this example as it's published, but we have other dev branches in prep). Leads into topics discussed in worksahop...

#### TO ADD Questions for explorations:  
## We can ask the class to break up into groups to decide on a question each to address together and report back on.
## Some suggestions:
## Does the climate change enabled model fit the data any better through time?
## Do all species responf in the same way to the combined effects of fishing and climate change forcings?
## How quickly do different fish recover from fishing?
## What happens when we add species that are not fish species?




